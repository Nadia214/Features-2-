# Features-2-
## üß† Feature Optimization Techniques ‚Äî With Regression Focus

---

### 1Ô∏è‚É£ Feature Selection

Choosing a subset of the original features that are relevant to the target variable.

#### üîπ A. Filter Methods (Statistical; model-free)

| Method                 | Use Case                                            | Regression? | Notes                                                    |
| ---------------------- | --------------------------------------------------- | ----------- | -------------------------------------------------------- |
| VarianceThreshold      | Remove low-variance features                        | ‚úÖ           | Simple, preprocessing step                               |
| SelectKBest            | Score-based selection                               | ‚úÖ/‚ùå         | Use `f_regression` (‚úÖ) or `chi2` (‚ùå classification only) |
| SelectPercentile       | Top % features                                      | ‚úÖ/‚ùå         | Based on scoring function                                |
| Correlation Threshold  | Drop weakly correlated or highly collinear features | ‚úÖ           | Based on Pearson/Spearman correlation                    |
| Mutual Information     | Non-linear dependency                               | ‚úÖ           | Use `mutual_info_regression`                             |
| ANOVA F-test           | Feature-class association                           | ‚ùå           | For classification only                                  |
| **mRMR**               | Max relevance, min redundancy                       | ‚úÖ           | Use `pymrmr`, discretizes features                       |
| **Feature Clustering** | Reduce correlated redundancy                        | ‚úÖ           | Cluster similar features and keep representative         |

#### üîπ B. Wrapper Methods (Model-based search)

| Method                              | Use Case                     | Regression? | Notes                             |
| ----------------------------------- | ---------------------------- | ----------- | --------------------------------- |
| RFE (Recursive Feature Elimination) | Recursive pruning            | ‚úÖ           | Needs a regressor (e.g., SVR, RF) |
| RFECV                               | RFE + cross-validation       | ‚úÖ           | Finds optimal #features           |
| Forward Selection                   | Adds one-by-one              | ‚úÖ           | Greedy, slow on large sets        |
| Backward Elimination                | Starts with all, removes     | ‚úÖ           | Inverse of forward                |
| SFS (Sequential FS)                 | Generalized forward/backward | ‚úÖ           | Can control scoring metric        |

#### üîπ C. Embedded Methods (Feature selection inside training)

| Method              | Use Case                                            | Regression? | Notes                               |
| ------------------- | --------------------------------------------------- | ----------- | ----------------------------------- |
| Lasso Regression    | Regularization-based                                | ‚úÖ           | L1 penalty forces sparsity          |
| ElasticNet          | L1 + L2 penalty                                     | ‚úÖ           | Better for correlated features      |
| Ridge Regression    | L2 regularization                                   | ‚úÖ           | Coefficients shrink but not to zero |
| Tree-based models   | Use splits to derive importances                    | ‚úÖ           | RF, XGBoost, LightGBM               |
| Logistic Regression | Classification only                                 | ‚ùå           | Not applicable for regression       |
| SelectFromModel     | Use estimator's `.coef_` or `.feature_importances_` | ‚úÖ           | Model-agnostic selector             |

#### üîπ D. Advanced / Hybrid Methods

| Method                  | Use Case                         | Regression? | Notes                              |
| ----------------------- | -------------------------------- | ----------- | ---------------------------------- |
| Boruta                  | All-relevant feature selection   | ‚úÖ           | RandomForest-based wrapper         |
| SHAP-based selection    | Keep top features by SHAP values | ‚úÖ           | Model-agnostic, local & global     |
| Permutation Importance  | Drop-shuffle-test                | ‚úÖ           | Interpretation-friendly            |
| Genetic Algorithms      | Evolution-based search           | ‚úÖ           | Costly, used in AutoML or research |
| **Stability Selection** | Robust under sampling noise      | ‚úÖ           | Combines Lasso + bootstrapping     |

---

### 2Ô∏è‚É£ Feature Reduction

Reduce dimensionality by transforming features into a smaller set of derived values.

#### üîπ A. Unsupervised Reduction

| Method       | Use Case                            | Regression? | Notes                          |
| ------------ | ----------------------------------- | ----------- | ------------------------------ |
| PCA          | Projects to max variance directions | ‚úÖ           | Linear, fast                   |
| Kernel PCA   | Non-linear projection               | ‚úÖ           | Expensive but powerful         |
| t-SNE        | Visualization only                  | ‚ö†Ô∏è          | Don't use for modeling         |
| UMAP         | Visualization                       | ‚ö†Ô∏è          | Faster alt to t-SNE            |
| TruncatedSVD | Sparse PCA                          | ‚úÖ           | Faster for sparse matrices     |
| ICA          | Independent signal separation       | ‚úÖ           | Often used in EEG, signal data |
| Autoencoders | Nonlinear compression               | ‚úÖ           | Deep learning-based            |

#### üîπ B. Supervised Reduction

| Method                                  | Use Case                    | Regression? | Notes                               |
| --------------------------------------- | --------------------------- | ----------- | ----------------------------------- |
| PLSR (Partial Least Squares Regression) | Preserve X‚Äìy correlation    | ‚úÖ           | Good for few samples, many features |
| LDA (Linear Discriminant Analysis)      | Maximize class separability | ‚ùå           | Classification only                 |
| Supervised PCA                          | PCA guided by target        | ‚úÖ           | Improves upon PCA                   |

---

### 3Ô∏è‚É£ Feature Importance Interpretation

Use to interpret or explain model behavior ‚Äî not for selection directly.

| Method                       | Use Case                     | Regression? | Notes                          |
| ---------------------------- | ---------------------------- | ----------- | ------------------------------ |
| Coefficients (Linear Models) | Sign & magnitude matter      | ‚úÖ           | Only for linear models         |
| Tree-based Importances       | Based on splits              | ‚úÖ           | `.feature_importances_`        |
| Permutation Importance       | Shuffle-drop-test            | ‚úÖ           | Works on any model             |
| SHAP Values                  | Local + global explanations  | ‚úÖ           | Best for interpretability      |
| LIME                         | Local explanations           | ‚úÖ           | One prediction at a time       |
| Drop Column Importance       | Retrain without one feature  | ‚úÖ           | Accurate but slow              |
| Partial Dependence Plots     | Feature effect visualization | ‚úÖ           | Not for selection, for insight |

---

### ‚úÖ Practical Tips

* ‚úÖ Combine methods: Use `SelectKBest ‚ûù Lasso ‚ûù SHAP` for robust pipelines.
* ‚úÖ Use feature clustering or mRMR before tree-based models to reduce redundancy.
* ‚ö†Ô∏è Avoid t-SNE, UMAP, or LDA for regression tasks.
* üß™ Use SHAP or Permutation Importance to **validate** feature selection decisions.

---
1. Linear Models (sklearn.linear_model)

| Model                          | Use Case            |
| ------------------------------ | ------------------- |
| **LinearRegression**           | Regression          |
| **Ridge**                      | Regression          |
| **Lasso**                      | Regression          |
| **ElasticNet**                 | Regression          |
| **BayesianRidge**              | Regression          |
| **HuberRegressor**             | Regression (robust) |
| **PassiveAggressiveRegressor** | Regression          |
| **TheilSenRegressor**          | Regression (robust) |
| **RANSACRegressor**            | Regression (robust) |
| **SGDRegressor**               | Regression          |

---

2. Tree-Based Models (sklearn.tree and sklearn.ensemble)

| Model                             | Use Case   |
| --------------------------------- | ---------- |
| **DecisionTreeRegressor**         | Regression |
| **RandomForestRegressor**         | Regression |
| **ExtraTreesRegressor**           | Regression |
| **GradientBoostingRegressor**     | Regression |
| **AdaBoostRegressor**             | Regression |
| **HistGradientBoostingRegressor** | Regression |



3. Support Vector & Neighbors

| Model                              | Use Case   |
| ---------------------------------- | ---------- |
| **SVR** (Support Vector Regressor) | Regression |
| **KNeighborsRegressor**            | Regression |


---
4. Neural Network

| Model            | Use Case   |
| ---------------- | ---------- |
| **MLPRegressor** | Regression |


---
5. Other Popular Libraries

| Model             | Use Case   |
| ----------------- | ---------- |
| **XGBRegressor**  | Regression |
| **LGBMRegressor** | Regression |


---


